{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "import torch\n",
    "\n",
    "# 向量\n",
    "x = torch.arange(4) # 创建向量 只有第0维(设有4个元素) 注意要和一行n列的矩阵区分开(矩阵有第1维)\n",
    "print(len(x)) # 访问向量的长度(元素个数)\n",
    "\n",
    "#矩阵\n",
    "y = x.reshape(2,2) # 把y设置为2*2的矩阵\n",
    "print(y.numel()) # 访问张量的元素总个数\n",
    "print(y.shape) # 矩阵在各个维(从0开始)上的长度\n",
    "print(y,y.T)  # 矩阵的转置 .T"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n",
      "4\n",
      "torch.Size([2, 2])\n",
      "tensor([[0, 1],\n",
      "        [2, 3]]) tensor([[0, 2],\n",
      "        [1, 3]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 张量\n",
    "> 就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。张量（本小节中的“张量”指代数对象）为我们提供了描述具有任意数量轴的 n 维数组的通用方法。例如，向量是一阶张量，矩阵是二阶张量。张量用特殊字体的大写字母（例如， `X` 、 `Y` 和 `Z` ）表示，它们的索引机制（例如 $x_{ijk}$）与矩阵类似。当我们开始处理图像时，张量将变得更加重要，图像以 `n` 维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道（channel）轴，用于堆叠颜色通道（红色、绿色和蓝色）。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# 两个张量之间的运算\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B, A*B # 均为按元素操作运算"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]),\n",
       " tensor([[  0.,   1.,   4.,   9.],\n",
       "         [ 16.,  25.,  36.,  49.],\n",
       "         [ 64.,  81., 100., 121.],\n",
       "         [144., 169., 196., 225.],\n",
       "         [256., 289., 324., 361.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# 标量和张量之间的运算\n",
    "a = 2\n",
    "A,a + A, a*A  # 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [14., 15., 16., 17.],\n",
       "         [18., 19., 20., 21.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# 降维 即获得张量的一些统计数据\n",
    "\n",
    "# 默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量\n",
    "print(A, A.sum())  \n",
    "\n",
    "# 指定沿某个轴来求和\n",
    "A_sum_axis0 = A.sum(axis=0) # 沿0轴求和(竖直向下,把所有行求和 第0维压缩为一行)\n",
    "print(A_sum_axis0.shape)  # 由于输入矩阵沿0轴降维以生成输出向量，因此输入的轴0的维数在输出形状中丢失。\n",
    "\n",
    "# 指定多个降维的轴的顺序\n",
    "A.sum(axis=[0, 1])  # Same as A.sum()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor(190.)\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# 常用统计\n",
    "print(A)\n",
    "\n",
    "# 均值\n",
    "print(A.mean() == A.sum() / A.numel())\n",
    "\n",
    "print(A.mean(axis = 0))\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor(True)\n",
      "tensor([ 8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# 非降维求和 方便进行广播运算\n",
    "\n",
    "sum_A = A.sum(axis=1, keepdims=True)  # 求和 但不降维 方便进行广播\n",
    "print(A,'\\n',sum_A,'\\n', A / sum_A) # 求出每个元素占当前行总和的比重\n",
    "\n",
    "A.cumsum(axis=0)  # 沿0轴计算A元素的累积总和 总和矩阵 \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) \n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]]) \n",
      " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 点积\n",
    "即给定两个向量(列向量是向量的默认方向), $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{d}$，它们的点积为 $\\mathbf{x}^{\\top} \\mathbf{y}$ ,也就是一行乘一列 得到的是一个数: $\\mathbf{x}^{\\top} \\mathbf{y}=\\sum_{i=1}^{d} x_{i} y_{i}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "y = torch.ones(4,dtype=torch.int64)\n",
    "x,y,torch.dot(x,y) # 注意数据类型一致才能相乘"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([1, 1, 1, 1]), tensor(6))"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 矩阵-向量积\n",
    "即一个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 乘一个向量 $\\mathbf{x} \\in \\mathbb{R}^{n}$，首先将矩阵行分块$\n",
    "\\mathbf{A}=\\left[\\begin{array}{c} \n",
    "\\mathbf{a}_{1}^{\\top} \\\\\n",
    "\\mathbf{a}_{2}^{\\top} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}_{m}^{\\top} \\\\\n",
    "\\end{array}\\right]\n",
    "$\n",
    "\n",
    "则 \n",
    "\n",
    "$\n",
    "\\mathbf{A} \\mathbf{x}=\\left[\\begin{array}{c}\n",
    "\\mathbf{a}_{1}^{\\top} \\\\\n",
    "\\mathbf{a}_{2}^{\\top} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}_{m}^{\\top}\n",
    "\\end{array}\\right] \\mathbf{x}=\\left[\\begin{array}{c}\n",
    "\\mathbf{a}_{1}^{\\top} \\mathbf{x} \\\\\n",
    "\\mathbf{a}_{2}^{\\top} \\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}_{m}^{\\top} \\mathbf{x}\n",
    "\\end{array}\\right]\n",
    "$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "A,x,torch.mv(A,x) # 矩阵-向量积"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 矩阵乘法\n",
    "对于两个矩阵 $=\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ 和 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$ ：\n",
    "\n",
    "$\\mathbf{A}=\\left[\\begin{array}{cccc}a_{11} & a_{12} & \\cdots & a_{1 k} \\\\ a_{21} & a_{22} & \\cdots & a_{2 k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n 1} & a_{n 2} & \\cdots & a_{n k}\\end{array}\\right], \\quad \\mathbf{B}=\\left[\\begin{array}{cccc}b_{11} & b_{12} & \\cdots & b_{1 m} \\\\ b_{21} & b_{22} & \\cdots & b_{2 m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{k 1} & b_{k 2} & \\cdots & b_{k m}\\end{array}\\right]$\n",
    "\n",
    "将$A$行分块，$B$列分快，则有：\n",
    "\n",
    "$\\mathbf{C}=\\mathbf{A} \\mathbf{B}=\\left[\\begin{array}{c}\\mathbf{a}_{1}^{\\top} \\\\ \\mathbf{a}_{2}^{\\top} \\\\ \\vdots \\\\ \\mathbf{a}_{n}^{\\top}\\end{array}\\right]\\left[\\begin{array}{llll}\\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m}\\end{array}\\right]=\\left[\\begin{array}{cccc}\\mathbf{a}_{1}^{\\top} \\mathbf{b}_{1} & \\mathbf{a}_{1}^{\\top} \\mathbf{b}_{2} & \\cdots & \\mathbf{a}_{1}^{\\top} \\mathbf{b}_{m} \\\\ \\mathbf{a}_{2}^{\\top} \\mathbf{b}_{1} & \\mathbf{a}_{2}^{\\top} \\mathbf{b}_{2} & \\cdots & \\mathbf{a}_{2}^{\\top} \\mathbf{b}_{m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{a}_{n}^{\\top} \\mathbf{b}_{1} & \\mathbf{a}_{n}^{\\top} \\mathbf{b}_{2} & \\cdots & \\mathbf{a}_{n}^{\\top} \\mathbf{b}_{m}\\end{array}\\right]$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "B = torch.ones((4,3))\n",
    "A,B,torch.mm(A,B) # 矩阵乘法"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 范数\n",
    "\n",
    "一个向量的范数告诉我们一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小\n",
    "\n",
    "常用的范数为2范数\n",
    "对于一个向量$x$ 则其2范数为向量元素平方和的平方根\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_{2}=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}}\n",
    "$$\n",
    "\n",
    "注：对于2范数，常常省略下标2\n",
    "\n",
    "还有一个比较常用的为1范数，其为向量元素的绝对值之和:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right| .\n",
    "$$\n",
    "\n",
    "广义一般化 $p范数$：\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_{p}=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}\n",
    "$$\n",
    "\n",
    "对于矩阵常用的范数为弗罗贝尼乌斯范数（Frobenius norm）：矩阵元素平方方和的平方根(类似于向量的2范数)\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{X}\\|_{F}=\\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{i j}^{2}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "print(u,'\\n',torch.norm(u)) # 计算向量的2范数\n",
    "print(torch.abs(u).sum())  # 计算向量的1范数\n",
    "u = torch.ones((2,3))\n",
    "print(u,'\\n',torch.norm(u))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 3., -4.]) \n",
      " tensor(5.)\n",
      "tensor(7.)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " tensor(2.4495)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "> 标量、向量、矩阵和张量是线性代数中的基本数学对象。\n",
    "\n",
    "> 向量泛化自标量，矩阵泛化自向量。\n",
    "\n",
    "> 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。\n",
    "\n",
    "> 一个张量可以通过sum和mean沿指定的轴降低维度。\n",
    "\n",
    "> 两个矩阵的按元素乘法被称为他们的哈达玛积。它与矩阵乘法不同。\n",
    "\n",
    "> 在深度学习中，我们经常使用范数，如 L1 范数、 L2 范数和弗罗贝尼乌斯范数。\n",
    "\n",
    "> 我们可以对标量、向量、矩阵和张量执行各种操作。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小练习\n",
    "1. 证明一个矩阵 A 的转置的转置是$\\mathbf{A}:\\left(\\mathbf{A}^{\\top}\\right)^{\\top}=\\mathbf{A}$\n",
    "   \n",
    "   解答：显然成立，将A行分块,然后转置两次 自然复原\n",
    "\n",
    "\n",
    "2. 给出两个矩阵 A 和 B ，显示转置的和等于和的转置： $\\mathbf{A}^{\\top}+\\mathbf{B}^{\\top}=(\\mathbf{A}+\\mathbf{B})^{\\top}$\n",
    "   \n",
    "   解答：显然成立 \n",
    "\n",
    "\n",
    "3. 给定任意方矩阵$\\mathbf{A}, \\mathbf{A}+\\mathbf{A}^{\\top}$总是对称的吗? 为什么?\n",
    "   解答：是的，只需要证明$\\mathbf{A}+\\mathbf{A}^{\\top}$为对称矩阵即可"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 我们在本节中定义了形状（2,3,4）的张量X。len(X)的输出结果是什么？对于任意形状的张量X, len(X)是否总是对应于X特定轴的长度?这个轴是什么?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "X = torch.arange(24).reshape((2,3,4))\n",
    "len(X) # 输出的始终为张量第0维的长度"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 运行A/A.sum(axis=1)，看看会发生什么。你能分析原因吗？\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "# A, A/A.sum(axis=1)\n",
    "# RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1\n",
    "\n",
    "# 原因 A.sum(axis=1) 导致降维 无法广播\n",
    "# 解决方法 不降维求和(只是第1维的长度压缩为1)\n",
    "A,A/A.sum(axis=1, keepdims=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "         [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "         [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "         [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "         [0.2286, 0.2429, 0.2571, 0.2714]]))"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. 当你在曼哈顿的两点之间旅行时，你需要在坐标上走多远，也就是说，就大街和街道而言？你能斜着走吗？\n",
    "   \n",
    "   解答 求出两点坐标的曼哈顿距离即可\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. 考虑一个具有形状（2,3,4）的张量，在轴0,1,2上的求和输出是什么形状? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "x7 = torch.arange(24).reshape((2,3,4))\n",
    "x7, x7.sum(axis=0), x7.sum(axis=1), x7.sum(axis=2)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([[12, 14, 16, 18],\n",
       "         [20, 22, 24, 26],\n",
       "         [28, 30, 32, 34]]),\n",
       " tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]),\n",
       " tensor([[ 6, 22, 38],\n",
       "         [54, 70, 86]]))"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. 向linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?\n",
    "   \n",
    "   linalg=linear（线性）+algebra（代数），norm则表示范数。\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "x8 = torch.arange(24).reshape((2,3,4))\n",
    "x8 = x8.float()\n",
    "print(x8, '\\n',torch.linalg.norm(x8)) # 注意需要为浮点型张量\n",
    "# 即为 所有元素平方的和的平方根"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.]]]) \n",
      " tensor(65.7571)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cv': conda)"
  },
  "interpreter": {
   "hash": "3a14354be428db890b240766593904592cb63a8036262423e22f244f201c3e6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}