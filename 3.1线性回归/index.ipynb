{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 线性回归\n",
    "即自变量 $x$ 和因变量 $y$ 之间的关系是线性的，即 $y$ 可以表示为 $x$ 中元素的加权和，这里通常允许包含观测值的一些噪声\n",
    "\n",
    "### 实例：房屋价格\n",
    "\n",
    "$$\n",
    "\\text { price }=w_{\\text {area }} \\cdot \\text { area }+w_{\\text {age }} \\cdot \\text { age }+b\n",
    "$$\n",
    "其中的 $w_{\\text {area }}$ 和 $w_{\\text {age }}$ 称为 权重（weight），b称为偏置（bias），或称为偏移量（offset）、截距 (intercept) 。权重决定了每个特征对我们预测值的影响。偏置是指当所有特征都取值为0时，预测值应为多少。\n",
    "\n",
    "### 回归\n",
    "\n",
    "即给定一个数据集，我们的目标是寻找模型的权重 $w$ 和偏置 $b$ \n",
    "\n",
    "## 数学表示\n",
    "\n",
    "对于特征集合 $\\mathbf{X}$, 预测值 $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{n}$ 可以通过矩阵-向量乘法表示为：\n",
    "$$\n",
    "\\hat{\\mathbf{y}}=\\mathbf{X} \\mathbf{w}+b\n",
    "$$\n",
    "\n",
    "$\\mathbf{X}$ 的每一行是一个样本，每一列是一种特征\n",
    "\n",
    "## 损失函数\n",
    "- 平方误差\n",
    "    $$\n",
    "    l^{(i)}(\\mathbf{w}, b)=\\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}\n",
    "    $$\n",
    "    其中样本 $i$ 的预测值为 $\\hat{y}^{(i)}$ ，其相应的真实标签为 $y^{(i)}$ 。\n",
    "\n",
    "    对于整个数据集，即包含`n`个样本时，需要度量整体误差，即:\n",
    "    $$\n",
    "    L(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)^{2}\n",
    "    $$\n",
    "\n",
    "    因此训练模型的过程，等价于找到一组参数$\\left(\\mathbf{w}^{*}, b^{*}\\right)$，使得$L(\\mathbf{w}, b)$最小即可\n",
    "\n",
    "    注: 线性回归存在最小值的解析解(为数不多存在解析解的模型)\n",
    "- 小批量随机梯度下降\n",
    "  \n",
    "  > 即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。在许多任务上，那些难以优化的模型效果要更好。因此，弄清楚如何训练这些难以优化的模型是非常重要的。\n",
    "  > 有一种名为梯度下降（gradient descent）的方法，这种方法几乎可以优化所有深度学习模型。它通过不断地在损失函数递减的方向上更新参数来降低误差。\n",
    "\n",
    "  \n",
    "  由于求梯度的需要很多算力消耗，因此我们一般不会直接对全体数据集直接求解梯度，而是在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）\n",
    "\n",
    "  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch \n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 避免使用for循环 使用矢量化加速\n",
    "n = 10000\n",
    "a = torch.ones(n)\n",
    "b = torch.ones(n)\n",
    "c = torch.zeros(n)\n",
    "\n",
    "# 对比计算c = a + b的耗时\n",
    "timer = d2l.Timer() # 自定义的运行时间测试类 此处开始启动\n",
    "\n",
    "# 普通for循环\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "print(f'{timer.stop():.5f} sec')\n",
    "\n",
    "# 线性\n",
    "timer.start()\n",
    "d = a + b\n",
    "print(f'{timer.stop():.5f} sec')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.08695 sec\n",
      "0.00012 sec\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cv': conda)"
  },
  "interpreter": {
   "hash": "3a14354be428db890b240766593904592cb63a8036262423e22f244f201c3e6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}