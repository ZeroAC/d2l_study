{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 求导拓展到一般形式(向量)\n",
    "・标量链式法则\n",
    "$$\n",
    "y=f(u), u=g(x) \\quad \\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x}\n",
    "$$\n",
    "・拓展到向量\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial \\mathbf{x}} \\quad \\frac{\\partial y}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} \\quad \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\n",
    "$$\n",
    "\n",
    "### 标量对向量求导的例子：\n",
    "对于\n",
    "$\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}，\\quad y \\in \\mathbb{R}$ \n",
    "，目标函数$z=(\\langle\\mathbf{x}, \\mathbf{w}\\rangle-y)^{2}$\n",
    "\n",
    "求 $\\frac{\\partial z}{\\partial \\mathbf{w}}$\n",
    "\n",
    "解：\n",
    "令\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&a=\\langle\\mathbf{x}, \\mathbf{w}\\rangle \\\\\n",
    "&b=a-y \\\\\n",
    "&z=b^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "则由链式求导法则可知：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial z}{\\partial \\mathbf{w}} &=\\frac{\\partial z}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial \\mathbf{w}} \\\\\n",
    "&=\\frac{\\partial b^{2}}{\\partial b} \\frac{\\partial a-y}{\\partial a} \\frac{\\partial\\langle\\mathbf{x}, \\mathbf{w}\\rangle}{\\partial \\mathbf{w}} \\\\\n",
    "&=2 b \\cdot 1 \\cdot \\mathbf{x}^{T} \\\\\n",
    "&=2(\\langle\\mathbf{x}, \\mathbf{w}\\rangle-y) \\mathbf{x}^{T}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自动求导\n",
    "> 深度学习框架通过自动计算导数，即自动求导（automatic differentiation），来加快繁琐的求导工作。实际中，根据我们设计的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动求导使系统能够随后反向传播梯度。 这里，反向传播（backpropagate）只是意味着跟踪整个计算图，填充关于每个参数的偏导数。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "# y = 2*torch.dot(x,x) \n",
    "# 想要求标量y对x向量的梯度\n",
    "\n",
    "# 注：我们需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。因为我们经常会成千上万次地更新相同的参数\n",
    "\n",
    "x.requires_grad_(True)  # 等价于 `x = torch.arange(4.0, requires_grad=True)`\n",
    "x.grad  # 默认值是None 对x的梯度将会保存在这里面\n",
    "y = 2 * torch.dot(x, x)\n",
    "y.backward()  # 调用反向传播函数  可自动计算y关于x每个分量的梯度\n",
    "print(x,'\\n',y,'\\n',x.grad) # 可以得到梯度 y = 2x'*x的梯度应该为4x\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True) \n",
      " tensor(28., grad_fn=<MulBackward0>) \n",
      " tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "z = x.sum()\n",
    "z\n",
    "# z.backward() 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "# x.grad \n",
    "x.grad.zero_()\n",
    "z.backward()\n",
    "x.grad\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cv': conda)"
  },
  "interpreter": {
   "hash": "3a14354be428db890b240766593904592cb63a8036262423e22f244f201c3e6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}