{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# vscode中jupyter notebook使用技巧\n",
    "\n",
    "## cell 命令模式快捷键\n",
    "注: 进入cell命令模式 只需要按`ESC`即可 命令模式下不区分大小写\n",
    "- `方向键用来操作选中的单元格`\n",
    "- `Shift+Enter` : 运行本单元，选中或插入（最后一个Cell的时候）下个单元\n",
    "- `Ctrl+Enter` : 运行本单元\n",
    "- `Alt+Enter` : 运行本单元，在其下插入新单元\n",
    "- `Y` : 单元转入代码状态\n",
    "- `M`:单元转入markdown状态 （目前尚不支持R 原生状态）\n",
    "- `Enter` : 转入编辑模式\n",
    "- `A` : 在上方插入新单元\n",
    "- `B` : 在下方插入新单元\n",
    "- `DD` : 删除选中的单元\n",
    "- `L` : 转换行号\n",
    "- `Shift+Space` : 向上滚动\n",
    "- `Space` : 向下滚动\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 引入pytorch框架"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 张量的创建"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\n",
    "x = torch.arange(2,18,2) #创建行向量 从2到18 间隔为2\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(x.shape) #获取张量的各个维数\n",
    "\n",
    "newX = x.reshape((2,2,-1)) #更改x的维数 -1表示按照总元素数 自动推导最后一维的个数\n",
    "\n",
    "print(newX)\n",
    "\n",
    "y = torch.zeros((2,3,4)) #创建全零矩阵 维数分别为(2,3,4)\n",
    "\n",
    "print(y)\n",
    "\n",
    "z = torch.ones((2,3,4))\n",
    "\n",
    "print(z)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 2,  4,  6,  8, 10, 12, 14, 16])\n",
      "torch.Size([8])\n",
      "tensor([[[ 2,  4],\n",
      "         [ 6,  8]],\n",
      "\n",
      "        [[10, 12],\n",
      "         [14, 16]]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 张量的运算"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "x = torch.Tensor([1.0,2,4,8])\n",
    "y = torch.Tensor([2,2,2,2])\n",
    "\n",
    "# 以下运算相当于对同一位置的元素进行该运算\n",
    "x + y, x - y, x * y, x / y, x ** y  # **表示求幂运算\n",
    "\n",
    "# 张量的连接\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4)) #3行4列\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) #3行4列\n",
    "torch.cat((X, Y), dim=0),torch.cat((X, Y), dim=1) #按行(第0维)连接 按列(第1维)连接\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# 张量的bool型运算\n",
    "X==Y # 形成同维bool矩阵 按元素比较"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# 广播机制 即对于不同维的张量也是可以进行运算的\n",
    "# 运算之前将张量扩充为同维矩阵 再进行运算\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a,b) # 分别为3行1列 一行2列\n",
    "           # 进行运算时会同时扩充为3行2列(复制型扩充)\n",
    "print(a+b) # 具体来说 矩阵a将复制列，矩阵b将复制行，然后再按元素相加"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]]) tensor([[0, 1]])\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 索引和切片\n",
    "### 注： py里的区间表示的是左闭右开  -1表示最后一个元素 -2 表示倒数第二个 以此类推"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "X,X[1:3],X[1:],X[:1],X[0:3,2:4]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[0., 1., 2., 3.]]),\n",
       " tensor([[ 2.,  3.],\n",
       "         [ 6.,  7.],\n",
       "         [10., 11.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 节省内存\n",
    "例如执行 $$ Y = Y + X $$ \n",
    "Python首先会计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置，因此并不会原地运行,会产生新的开辟内存空间消耗\n",
    "> 这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新。其次，我们可能通过多个变量指向相同参数。如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# 验证是否原地操作\n",
    "before = id(Y) # id会显示Y的地址\n",
    "Y = Y + X\n",
    "id(Y) == before # 不相等 说明开辟了新的内存空间 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# 解决原地操作问题 使用切片赋值\n",
    "before = id(X)\n",
    "X += Y # 等价于 X[:] = X+Y\n",
    "id(X) == before"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "> 深度学习存储和操作数据的主要接口是张量（ n 维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小练习\n",
    "1. 运行本节中的代码。将本节中的条件语句X == Y更改为X < Y或X > Y，然后看看你可以得到什么样的张量。\n",
    "\n",
    "2. 用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# 练习1\n",
    "X,Y,X<Y,X>Y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 2.,  3.,  8.,  9.],\n",
       "         [ 9., 12., 15., 18.],\n",
       "         [20., 21., 22., 23.]]),\n",
       " tensor([[ 2.,  2.,  6.,  6.],\n",
       "         [ 5.,  7.,  9., 11.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor([[False,  True,  True,  True],\n",
       "         [ True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True]]))"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "练习2 [查阅文档可知](https://pytorch.apachecn.org/docs/1.0/notes_broadcasting.html) \n",
    "## PyTorch操作支持广播，它的Tensor参数可以自动扩展为相同的类型大小(不需要复制数据）。\n",
    "\n",
    "### 一般语义\n",
    "#### 如果遵守以下规则，则两个张量是“可广播的”：\n",
    "\n",
    "- 每个张量至少有一个维度；\n",
    "- 遍历张量维度大小时，从末尾开始遍历，两个张量的维度大小要么相等,要么其中一个1或者是不存在。 全部符合即可\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# 练习2\n",
    "a1 = torch.arange(6).reshape((2,3,1))\n",
    "a2 = torch.arange(6).reshape((3,2)) # 从维度的末尾开始比较\n",
    "a1,a2,a1+a2 #相加时 会将a1,a2扩充为(2,3,2)维"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[0],\n",
       "          [1],\n",
       "          [2]],\n",
       " \n",
       "         [[3],\n",
       "          [4],\n",
       "          [5]]]),\n",
       " tensor([[0, 1],\n",
       "         [2, 3],\n",
       "         [4, 5]]),\n",
       " tensor([[[ 0,  1],\n",
       "          [ 3,  4],\n",
       "          [ 6,  7]],\n",
       " \n",
       "         [[ 3,  4],\n",
       "          [ 6,  7],\n",
       "          [ 9, 10]]]))"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cv': conda)"
  },
  "interpreter": {
   "hash": "3a14354be428db890b240766593904592cb63a8036262423e22f244f201c3e6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}